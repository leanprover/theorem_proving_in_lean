#+Title: Theorem Proving in Lean
#+Author: [[http://www.andrew.cmu.edu/user/avigad][Jeremy Avigad]], [[http://leodemoura.github.io][Leonardo de Moura]]

* Inductive Types
:PROPERTIES:
  :CUSTOM_ID: Inductive_Types
:END:

We have seen that Lean's formal foundation includes basic types,
=Prop, Type 1, Type 2, ...=, and allows for the formation of
dependent function types, =Π x : α. β=. In the examples, we have also
made use of additional types like =bool=, =nat=, and =int=, and type
constructors, like =list=, and product, =×=. In fact, in Lean's library,
every concrete type other than the universes and every type
constructor other than Pi is an instance of a general family of type
constructions known as /inductive types/. It is remarkable that it is
possible to construct a substantial edifice of mathematics based on
nothing more than the type universes, Pi types, and inductive types;
everything else follows from those.

Intuitively, an inductive type is built up from a specified list of
constructors. In Lean, the syntax for specifying such a type is as follows:
#+BEGIN_SRC text
inductive foo : Type
| constructor₁ : ... → foo
| constructor₂ : ... → foo
...
| constructorₙ : ... → foo
#+END_SRC
The intuition is that each constructor specifies a way of building new
objects of =foo=, possibly from previously constructed values. The type
=foo= consists of nothing more than the objects that are constructed in
this way. The first character =|= in an inductive declaration is optional.
We can also separate constructors using a comma instead of =|=.

We will see below that the arguments to the constructors can include
objects of type =foo=, subject to a certain "positivity" constraint,
which guarantees that elements of =foo= are built from the bottom
up. Roughly speaking, each =...= can be any Pi type constructed from
=foo= and previously defined types, in which =foo= appears, if at all,
only as the "target" of the Pi type. For more details, see \cite{dybjer:94}.

We will provide a number of examples of inductive types. We will also
consider slight generalizations of the scheme above, to mutually
defined inductive types, and so-called /inductive families/.

As with the logical connectives, every inductive type comes with
introduction rules, which show how to construct an element of the
type, and elimination rules, which show how to "use" an element of the
type in another construction. The analogy to the logical connectives
should not come as a surprise; as we will see below, they, too, are
examples of inductive type constructions. You have already seen the
introduction rules for an inductive type: they are just the
constructors that are specified in the definition of the type. The
elimination rules provide for a principle of recursion on the type,
which includes, as a special case, a principle of induction as well.

In the next chapter, we will describe Lean's function definition
package, which provides even more convenient ways to define functions
on inductive types and carry out inductive proofs. But because the
notion of an inductive type is so fundamental, we feel it is important
to start with a low-level, hands-on understanding. We will start with
some basic examples of inductive types, and work our way up to more
elaborate and complex examples.

** Enumerated Types

The simplest kind of inductive type is simply a type with a finite,
enumerated list of elements.
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday
#+END_SRC
The =inductive= command creates a new type, =weekday=. The
constructors all live in the =weekday= namespace.
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

-- BEGIN
check weekday.sunday
check weekday.monday

open weekday

check sunday
check monday
-- END
#+END_SRC
Think of =sunday=, =monday=, ..., =saturday= as being distinct elements of
=weekday=, with no other distinguishing properties. The elimination
principle, =weekday.rec=, is defined along with the type
=weekday= and its constructors. It is also known as a /recursor/, and
it is what makes the type "inductive": it allows us to define a
function on =weekday= by assigning values corresponding to each
constructor. The intuition is that an inductive type is exhaustively
generated by the constructors, and has no elements beyond those they
construct.

We will use a slight variant of =weekday.rec=, =weekday.rec_on= (also
generated automatically), which takes its arguments in a more convenient
order. (Note that the shorter names =rec= and =rec_on= are not made
available by default when we open the =weekday= namespace. This avoids
clashes with the functions of the same names for other inductive types.)
If we import =nat=, we can use =weekday.rec_on= to define a function
from =weekday= to the natural numbers:
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

-- BEGIN
def number_of_day (d : weekday) : ℕ :=
weekday.rec_on d 1 2 3 4 5 6 7

eval number_of_day weekday.sunday
eval number_of_day weekday.monday
eval number_of_day weekday.tuesday
-- END
#+END_SRC
The first (explicit) argument to =rec_on= is the element being "analyzed." The
next seven arguments are the values corresponding to the seven
constructors. Note that =number_of_day weekday.sunday= evaluates to
=1=: the computation rule for =rec_on= recognizes that =sunday= is a
constructor, and returns the appropriate argument.

Below we will encounter a more restricted variant of =rec_on=, namely,
=cases_on=. When it comes to enumerated types, =rec_on= and =cases_on=
are the same. You may prefer to use the label =cases_on=, because it
emphasizes that the definition is really a definition by cases.
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

-- BEGIN
def number_of_day (d : weekday) : ℕ :=
weekday.cases_on d 1 2 3 4 5 6 7
-- END
#+END_SRC

It is often useful to group definitions and theorems related to a
structure in a namespace with the same name. For example, we can put
the =number_of_day= function in the =weekday= namespace. We are then
allowed to use the shorter name when we open the namespace.

The names =rec_on=, =cases_on=, =induction_on=, and so on are
generated automatically. As noted above, they are /protected/ to avoid
name clashes. In other words, they are not provided by default when
the namespace is opened. However, you can explicitly declare
abbreviations for them using the =renaming= option when you open a
namespace.

# TODO: we haven't discussed the [reducible] attribute yet
# TODO: we haven't discussed open (renaming ...)

#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

-- BEGIN
namespace weekday
  @[reducible]
  private def cases_on := @weekday.cases_on

  def number_of_day (d : weekday) : nat :=
  cases_on d 1 2 3 4 5 6 7
end weekday

eval weekday.number_of_day weekday.sunday

open weekday (renaming cases_on → cases_on)

eval number_of_day sunday
check cases_on
-- END
#+END_SRC
We can define functions from =weekday= to =weekday=:
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

-- BEGIN
namespace weekday
  def next (d : weekday) : weekday :=
  weekday.cases_on d monday tuesday wednesday thursday friday saturday sunday

  def previous (d : weekday) : weekday :=
  weekday.cases_on d saturday sunday monday tuesday wednesday thursday friday

  eval next (next tuesday)
  eval next (previous tuesday)

  example : next (previous tuesday) = tuesday := rfl
end weekday
-- END
#+END_SRC

How can we prove the general theorem that =next (previous d) = d= for
any weekday =d=? The induction principle parallels the recursion
principle: we simply have to provide a proof of the claim for each
constructor:
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

namespace weekday
  def next (d : weekday) : weekday :=
  weekday.cases_on d monday tuesday wednesday thursday friday saturday sunday

  def previous (d : weekday) : weekday :=
  weekday.cases_on d saturday sunday monday tuesday wednesday thursday friday

-- BEGIN
  theorem next_previous (d: weekday) : next (previous d) = d :=
  weekday.induction_on d
    (show next (previous sunday) = sunday, from rfl)
    (show next (previous monday) = monday, from rfl)
    (show next (previous tuesday) = tuesday, from rfl)
    (show next (previous wednesday) = wednesday, from rfl)
    (show next (previous thursday) = thursday, from rfl)
    (show next (previous friday) = friday, from rfl)
    (show next (previous saturday) = saturday, from rfl)
-- END
end weekday
#+END_SRC

In fact, =induction_on= is just a special case of =rec_on= where the
target type is an element of =Prop=. In other words, under the
propositions-as-types correspondence, the principle of induction is a
type of definition by recursion, where what is being "defined" is a
proof instead of a piece of data. We could equally well have used
=cases_on=:
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

namespace weekday
  def next (d : weekday) : weekday :=
  weekday.cases_on d monday tuesday wednesday thursday friday saturday sunday

  def previous (d : weekday) : weekday :=
  weekday.cases_on d saturday sunday monday tuesday wednesday thursday friday

-- BEGIN
  theorem next_previous (d: weekday) : next (previous d) = d :=
  weekday.cases_on d
    (show next (previous sunday) = sunday, from rfl)
    (show next (previous monday) = monday, from rfl)
    (show next (previous tuesday) = tuesday, from rfl)
    (show next (previous wednesday) = wednesday, from rfl)
    (show next (previous thursday) = thursday, from rfl)
    (show next (previous friday) = friday, from rfl)
    (show next (previous saturday) = saturday, from rfl)
-- END
end weekday
#+END_SRC
While the =show= commands make the proof clearer and more
readable, they are not necessary:
#+BEGIN_SRC lean
inductive weekday : Type
| sunday : weekday
| monday : weekday
| tuesday : weekday
| wednesday : weekday
| thursday : weekday
| friday : weekday
| saturday : weekday

namespace weekday
  def next (d : weekday) : weekday :=
  weekday.cases_on d monday tuesday wednesday thursday friday saturday sunday

  def previous (d : weekday) : weekday :=
  weekday.cases_on d saturday sunday monday tuesday wednesday thursday friday

-- BEGIN
  theorem next_previous (d: weekday) : next (previous d) = d :=
  weekday.cases_on d rfl rfl rfl rfl rfl rfl rfl
-- END
end weekday
#+END_SRC

Some fundamental data types in the Lean library are instances of
enumerated types.
#+BEGIN_SRC lean
import standard

namespace hide

-- BEGIN
inductive empty : Type

inductive unit : Type
| star : unit

inductive bool : Type
| ff : bool
| tt : bool
-- END

end hide
#+END_SRC
(To run these examples, we put them in a namespace called =hide=, so
that a name like =bool= does not conflict with the =bool= in the
standard library. This is necessary because these types are part of
the Lean "prelude" that is automatically imported with the system is
started.)

The type =empty= is an inductive data type with no constructors. The
type =unit= has a single element, =star=, and the type =bool=
represents the familiar boolean values. As an exercise, you should
think about what the introduction and elimination rules for these
types do. As a further exercise, we suggest defining boolean
operations =band=, =bor=, =bnot= on the boolean, and verifying common
identities. Note that defining a binary operation like =band= will
require nested cases splits:
#+BEGIN_SRC lean
namespace hide

-- BEGIN
def band (b1 b2 : bool) : bool :=
bool.cases_on b1
  ff
  (bool.cases_on b2 ff tt)
-- END

end hide
#+END_SRC
Similarly, most identities can be proved by introducing suitable case
splits, and then using =rfl=.

** Constructors with Arguments

Enumerated types are a very special case of inductive types, in which
the constructors take no arguments at all. In general, a
"construction" can depend on data, which is then represented in the
constructed argument. Consider the definitions of the product type and
sum type in the library:
#+BEGIN_SRC lean
namespace hide

-- BEGIN
universe variables u v

inductive prod (α : Type u) (β : Type v)
| mk : α → β → prod

inductive sum (α : Type u) (β : Type v)
| inl {} : α → sum
| inr {} : β → sum
-- END

end hide
#+END_SRC
Notice that we do not include the types =α= and =β= in the target of
the constructors. For the moment, ignore the annotation ={}= after the
constructors =inl= and =inr=; we will explain that below. In the
meanwhile, think about what is going on in these examples. The product
type has one constructor, =prod.mk=, which takes two arguments. To
define a function on =prod α β=, we can assume the input is of the
form =prod.mk a b=, and we have to specify the output, in terms of =a=
and =b=. We can use this to define the two projections for prod;
remember that the standard library defines notation =α × β= for =prod
α β= and =(a, b)= for =prod.mk a b=.
#+BEGIN_SRC lean
universe variables u v

-- BEGIN
def fst {α : Type u} {β : Type v} (p : α × β) : α :=
prod.rec_on p (λ a b, a)

def snd {α : Type u} {β : Type v} (p : α × β) : β :=
prod.rec_on p (λ a b, b)
-- END
#+END_SRC
The function =fst= takes a pair, =p=. Applying the recursor
=prod.rec_on p (fun a b, a)= interprets =p= as a pair, =prod.mk a b=,
and then uses the second argument to determine what to do with =a= and
=b=. Remember that you can enter the symbol for a product by typing
=\times=. Recall also from [[file:02_Dependent_Type_Theory.org::#Dependent_Types][Section 2.8]] that to give these definitions
the greatest generality possible, we allow the types =α= and =β= to
belong to any universe.

Here is another example:
#+BEGIN_SRC lean
def prod_example (p : bool × ℕ) : ℕ :=
prod.rec_on p (λ b n, cond b (2 * n) (2 * n + 1))

eval prod_example (tt, 3)
eval prod_example (ff, 3)
#+END_SRC
The =cond= function is a boolean conditional: =cond b t1 t2= return
=t1= if =b= is true, and =t2= otherwise. (It has the same effect as
=bool.rec_on b t2 t1=.) The function =prod_example= takes a pair
consisting of a boolean, =b=, and a number, =n=, and returns either
=2 * n= or =2 * n + 1= according to whether =b= is true or false.

In contrast, the sum type has /two/ constructors, =inl= and =inr= (for
"insert left" and "insert right"), each of which takes /one/ (explicit)
argument. To define a function on =sum α β=, we have to handle two
cases: either the input is of the form =inl a=, in which case we have
to specify an output value in terms of =a=, or the input is of the
form =inr b=, in which case we have to specify an output value in
terms of =b=.
#+BEGIN_SRC lean
-- BEGIN
def sum_example (s : ℕ ⊕ ℕ) : ℕ :=
sum.cases_on s (λ n, 2 * n) (λ n, 2 * n + 1)

eval sum_example (sum.inl 3)
eval sum_example (sum.inr 3)
-- END
#+END_SRC
This example is similar to the previous one, but now an input to
=sum_example= is implicitly either of the form =inl n= or =inr n=. In
the first case, the function returns =2 * n=, and the second case, it
returns =2 * n + 1=. You can enter the symbol for the sum by typing
=\oplus=.

In the section after next we will see what happens when the
constructor of an inductive type takes arguments from the inductive
type itself. What characterizes the examples we consider in this
section is that this is not the case: each constructor relies only on
previously specified types.

Notice that a type with multiple constructors is disjunctive: an
element of =sum α β= is either of the form =inl a= /or/ of the form
=inl b=. A constructor with multiple arguments introduces conjunctive
information: from an element =prod.mk a b= of =prod α β= we can
extract =a= /and/ =b=. An arbitrary inductive type can include both
features, by having any number of constructors, each of which takes
any number of arguments.

A type, like =prod=, with only one constructor is purely conjunctive:
the constructor simply packs the list of arguments into a single piece
of data, essentially a tuple where the type of subsequent arguments
can depend on the type of the initial argument. We can also think of
such a type as a "record" or a "structure". In Lean, these two words
are synonymous, and provide alternative syntax for inductive types
with a single constructor.
#+BEGIN_SRC lean
namespace hide

-- BEGIN
structure prod (α β : Type) :=
mk :: (fst : α) (snd : β)
-- END

end hide
#+END_SRC
The =structure= command simultaneously introduces the inductive type,
=prod=, its constructor, =mk=, the usual eliminators (=rec=,
=rec_on=), as well as the projections, =fst= and =snd=, as defined
above.

If you do not name the constructor, Lean uses =mk= as a
default. For example, the following defines a record to store a color
as a triple of RGB values:
#+BEGIN_SRC lean
open nat

-- BEGIN
record color := (red : nat) (green : nat) (blue : nat)
def yellow := color.mk 255 255 0
eval color.red yellow
-- END
#+END_SRC
The definition of =yellow= forms the record with the three values
shown, and the projection =color.red= returns the red component. The
=structure= command is especially useful for defining algebraic
structures, and Lean provides substantial infrastructure to support
working with them. Here, for example, is the definition of a
semigroup:
#+BEGIN_SRC lean
universe variable u

structure Semigroup :=
(carrier : Type u)
(mul : carrier → carrier → carrier)
(mul_assoc : ∀ a b c, mul (mul a b) c = mul a (mul b c))
#+END_SRC
We will see more examples in [[file:09_Structures_and_Records.org::#Structures_and_Records][Chapter 9]].

Notice that the product type depends on parameters =α β : Type= which
are arguments to the constructors as well as =prod=.  Lean detects
when these arguments can be inferred from later arguments to a
constructor, and makes them implicit in that case. Sometimes an
argument can only be inferred from the return type, which means that
it could not be inferred by parsing the expression from bottom up, but
may be inferrable from context. In that case, Lean does not make the
argument implicit by default, but will do so if we add the annotation
={}= after the constructor. We used that option, for example, in the
definition of =sum=:
#+BEGIN_SRC lean
universe variables u v

namespace hide

-- BEGIN
inductive sum (α : Type u) (β : Type v)
| inl {} : α → sum
| inr {} : β → sum
-- END

end hide
#+END_SRC
αs a result, the argument =α= to =inl= and the argument =β= to
=inr= are left implicit.

We have already discussed sigma types, also known as the dependent
product:
#+BEGIN_SRC lean
universe variables u v

namespace hide

-- BEGIN
inductive sigma {α : Type u} (β : α → Type v)
| dpair : Π a : α, β a → sigma
-- END

end hide
#+END_SRC
Two more examples of inductive types in the library are the
following:
#+BEGIN_SRC lean
universe variable u

namespace hide

-- BEGIN
inductive option (α : Type u)
| none {} : option
| some    : α → option

inductive inhabited (α : Type u)
| mk : α → inhabited
-- END

end hide
#+END_SRC
In the semantics of dependent type theory, there is no built-in notion
of a partial function. Every element of a function type =α → β= or a
Pi type =Π x : α, β= is assumed to have a value at every input. The
=option= type provides a way of representing partial functions. An
element of =option β= is either =none= or of the form =some b=, for
some value =b : β=. Thus we can think of an element =f= of the type =α
→ option β= as being a partial function from =α= to =β=: for every
=a : α=, =f a= either returns =none=, indicating the =f a= is
"undefined", or =some b=.

An element of =inhabited α= is simply a witness to the fact that there
is an element of =α=. Later, we will see that =inhabited= is an
example of a /type class/ in Lean: Lean can be instructed that
suitable base types are inhabited, and can automatically infer that
other constructed types are inhabited on that basis.

As exercises, we encourage you to develop a notion of composition for
partial functions from =α= to =β= and =β= to =γ=, and show that it
behaves as expected. We also encourage you to show that =bool= and
=nat= are inhabited, that the product of two inhabited types is
inhabited, and that the type of functions to an inhabited type is
inhabited.

** Inductively Defined Propositions

Inductively defined types can live in any type universe, including the
bottom-most one, =Prop=. In fact, this is exactly how the logical
connectives are defined.
#+BEGIN_SRC lean
namespace hide

-- BEGIN
inductive false : Prop

inductive true : Prop
| intro : true

inductive and (a b : Prop) : Prop
| intro : a → b → and

inductive or (a b : Prop) : Prop
| intro_left  : a → or
| intro_right : b → or
-- END

end hide
#+END_SRC
You should think about how these give rise to the introduction and
elimination rules that you have already seen. There are rules that
govern what the eliminator of an inductive type can eliminate /to/,
that is, what kinds of types can be the target of a recursor. Roughly
speaking, what characterizes inductive types in =Prop= is that one can
only eliminate to other types in =Prop=. This is consistent with the
understanding that if =p : Prop=, an element =hp : p= carries no
data. There is a small exception to this rule, however, which we will
discuss below, in the section on inductive families.

# TODO: say something more about the universe rules?

Even the existential quantifier is inductively defined:
#+BEGIN_SRC lean
universe variable u

namespace hide

-- BEGIN
inductive Exists {α : Type u} (p : α → Prop) : Prop
| intro : ∀ (a : α), p a → Exists

def exists.intro := @Exists.intro
-- END

end hide
#+END_SRC
Keep in mind that the notation =∃ x : α, p= is syntactic sugar for
=Exists (λ x : α, p)=.

The definitions of =false=, =true=, =and=, and =or= are perfectly
analogous to the definitions of =empty=, =unit=, =prod=, and
=sum=. The difference is that the first group yields elements of
=Prop=, and the second yields elements of =Type i= for =i= greater
than 0. In a similar way, =∃ x : α, p= is a =Prop=-valued variant of
=Σ x : α, p=.

This is a good place to mention another inductive type, denoted ={x :
α | p}=, which is sort of a hybrid between =∃ x : α, P= and =Σ x : α, P=.
#+BEGIN_SRC lean
universe variable u

namespace hide

-- BEGIN
inductive subtype {α : Type u} (p : α → Prop)
| tag : Π x : α, p x → subtype
-- END

end hide
#+END_SRC
The notation ={x : α | p}= is syntactic sugar for =subtype (λ x : α,
p)=. It is modeled after subset notation in set theory: the idea is
that ={x : α | p}= denotes the collection of elements of =α= that have
property =p=.

** Defining the Natural Numbers

The inductively defined types we have seen so far are "flat":
constructors wrap data and insert it into a type, and the
corresponding recursor unpacks the data and acts on it. Things get
much more interesting when the constructors act on elements of the
very type being defined. A canonical example is the type =nat= of
natural numbers:
#+BEGIN_SRC lean
namespace hide

-- BEGIN
inductive nat : Type
| zero : nat
| succ : nat → nat
-- END

end hide
#+END_SRC
There are two constructors. We start with =zero : nat=; it takes no
arguments, so we have it from the start. In contrast, the constructor
=succ= can only be applied to a previously constructed =nat=. Applying
it to =zero= yields =succ zero : nat=. Applying it again yields =succ
(succ zero) : nat=, and so on. Intuitively, =nat= is the "smallest"
type with these constructors, meaning that it is exhaustively (and
freely) generated by starting with =zero= and applying =succ=
repeatedly.

As before, the recursor for =nat= is designed to define a dependent
function =f= from =nat= to any domain, that is, an element =f= of
=Π n : nat, C n= for some =C : nat → Type=. It has to handle two cases:
the case where the input is =zero=, and the case where the input is
of the form =succ n= for some =n : nat=. In the first case, we simply
specify a target value with the appropriate type, as before. In the
second case, however, the recursor can assume that a value of =f= at
=n= has already been computed. As a result, the next argument to the
recursor specifies a value for =f (succ n)= in terms of =n= and =f
n=. If we check the type of the recursor,
#+BEGIN_SRC lean
namespace hide

inductive nat : Type
| zero : nat
| succ : nat → nat
-- BEGIN
check @nat.rec_on
-- END

end hide
#+END_SRC
we find the following:
#+BEGIN_SRC text
  Π {C : nat → Type} (n : nat),
    C nat.zero → (Π (a : nat), C a → C (nat.succ a)) → C n
#+END_SRC
The implicit argument, =C=, is the codomain of the function being
defined. In type theory it is common to say =C= is the =motive= for
the elimination/recursion.  The next argument, =n : nat=, is the input
to the function. It is also known as the =major premise=. Finally, the
two arguments after specify how to compute the zero and successor
cases, as described above. They are also known as the =minor
premises=.

Consider, for example, the addition function =add m n= on the natural
numbers. Fixing =m=, we can define addition by recursion on =n=. In
the base case, we set =add m zero= to =m=. In the successor step,
assuming the value =add m n= is already determined, we define =add m
(succ n)= to be =succ (add m n)=.
#+BEGIN_SRC lean
namespace hide

inductive nat : Type
| zero : nat
| succ : nat → nat
-- BEGIN
namespace nat

def add (m n : nat) : nat :=
nat.rec_on n m (λ n add_m_n, succ add_m_n)

-- try it out
eval add (succ zero) (succ (succ zero))

end nat
-- END

end hide
#+END_SRC

It is useful to put such definitions into a namespace, =nat=. We can
then go on to define familiar notation in that namespace. The two
defining equations for addition now hold definitionally:
#+BEGIN_SRC lean
namespace hide

inductive nat : Type
| zero : nat
| succ : nat → nat

namespace nat

def add (m n : nat) : nat :=
nat.rec_on n m (fun n add_m_n, succ add_m_n)
-- BEGIN
instance : has_zero nat := has_zero.mk zero
instance : has_add nat := has_add.mk add

theorem add_zero (m : nat) : m + 0 = m := rfl
theorem add_succ (m n : nat) : m + succ n = succ (m + n) := rfl
-- END
end nat

end hide
#+END_SRC
We will explain how the =instance= command works in [[file:10_Type_Classes.org::#Type_Classes][Chapter 10]]. In the
examples below, we will henceforth use Lean's version of the natural
numbers.

Proving a fact like =0 + m = m=, however, requires a proof by
induction. As observed above, the induction principle is just a
special case of the recursion principle, when the codomain =C n= is an
element of =Prop=. It represents the familiar pattern of an inductive
proof: to prove =∀ n, C n=, first prove =C 0=, and then, for arbitrary
=n=, assume =ih : C n= and prove =C (succ n)=.
#+BEGIN_SRC lean
namespace hide
open nat

-- BEGIN
theorem zero_add (n : ℕ) : 0 + n = n :=
nat.induction_on n
  (show 0 + 0 = 0, from rfl)
  (take n,
    assume ih : 0 + n = n,
    show 0 + succ n = succ n, from
      calc
        0 + succ n = succ (0 + n) : rfl
          ... = succ n : by rw ih)

-- END
end hide
#+END_SRC

In the example above, we encourage you to replace =induction_on= with
=rec_on= and observe that the theorem is still accepted by Lean. As we
have seen above, =induction_on= is just a special case of =rec_on=.

For another example, let us prove the associativity of addition, =∀ m n
k, m + n + k = m + (n + k)=. (The notation =+=, as we have defined it,
associates to the left, so =m + n + k= is really =(m + n) + k=.) The
hardest part is figuring out which variable to do the induction
on. Since addition is defined by recursion on the second argument, =k=
is a good guess, and once we make that choice the proof almost writes
itself:
#+BEGIN_SRC lean
namespace hide
open nat

-- BEGIN
theorem add_assoc (m n k : ℕ) : m + n + k = m + (n + k) :=
nat.induction_on k
  (show m + n + 0 = m + (n + 0), from rfl)
  (take k,
    assume ih : m + n + k = m + (n + k),
    show m + n + succ k = m + (n + succ k), from
      calc
        m + n + succ k = succ (m + n + k) : rfl
          ... = succ (m + (n + k)) : by rw ih
          ... = m + succ (n + k) : rfl
          ... = m + (n + succ k) : rfl)
-- END
end hide
#+END_SRC

For another example, suppose we try to prove the commutativity of
addition. Choosing induction on the second argument, we might begin as
follows:
#+BEGIN_SRC lean
namespace hide
open nat

theorem add_assoc (m n k : ℕ) : m + n + k = m + (n + k) :=
nat.induction_on k
  (show m + n + 0 = m + (n + 0), from rfl)
  (take k,
    assume ih : m + n + k = m + (n + k),
    show m + n + succ k = m + (n + succ k), from
      calc
        m + n + succ k = succ (m + n + k) : rfl
          ... = succ (m + (n + k)) : by rw ih
          ... = m + succ (n + k) : rfl
          ... = m + (n + succ k) : rfl)

-- BEGIN
theorem add_comm (m n : nat) : m + n = n + m :=
nat.induction_on n
  (show m + 0 = 0 + m, by rw nat.zero_add)
  (take n,
    assume ih : m + n = n + m,
    calc
      m + succ n = succ (m + n) : rfl
        ... = succ (n + m) : by rw ih
        ... = succ n + m : sorry)
-- END

end hide
#+END_SRC
At this point, we see that we need another supporting fact, namely,
that =succ (n + m) = succ n + m=. We can prove this by induction on
=m=:
#+BEGIN_SRC lean
namespace hide
open nat

theorem add_assoc (m n k : ℕ) : m + n + k = m + (n + k) :=
nat.induction_on k
  (show m + n + 0 = m + (n + 0), from rfl)
  (take k,
    assume ih : m + n + k = m + (n + k),
    show m + n + succ k = m + (n + succ k), from
      calc
        m + n + succ k = succ (m + n + k) : rfl
          ... = succ (m + (n + k)) : by rw ih
          ... = m + succ (n + k) : rfl
          ... = m + (n + succ k) : rfl)

-- BEGIN
theorem succ_add (m n : nat) : succ m + n = succ (m + n) :=
nat.induction_on n
  (show succ m + 0 = succ (m + 0), from rfl)
  (take n,
    assume ih : succ m + n = succ (m + n),
    show succ m + succ n = succ (m + succ n), from
      calc
        succ m + succ n = succ (succ m + n) : rfl
          ... = succ (succ (m + n)) : by rw ih
          ... = succ (m + succ n) : rfl)
-- END
end hide
#+END_SRC
We can then replace the =sorry= in the previous proof with =succ_add=.

As an exercise, try defining other operations on the natural numbers,
such as multiplication, the predecessor function (with =pred 0 = 0=),
truncated subtraction (with =n - m = 0= when =m= is greater than or
equal to =n=), and exponentiation. Then try proving some of their
basic properties, building on the theorems we have already proved.


** Tactics

Given the fundamental importance of inductive types in Lean, it should
not be surprising that there are a number of tactics described to work
with them effectively. We describe some of them here.

The =cases= tactic works on elements of an inductively defined type,
and does what the name suggests: it decomposes the element according
to each of the possible constructors. In its most basic form, it is
applied to an element =x= in the local context. It then reduces the
goal to cases in which =x= is replaced by each of the constructions.
#+BEGIN_SRC lean
open nat
variable p : ℕ → Prop

example (hz : p 0) (hs : ∀ n, p (succ n)) : ∀ n, p n :=
begin
  intro n,
  cases n,
  { exact hz },  -- goal is p 0
  apply hs       -- goal is a : ℕ ⊢ p (succ a)
end
#+END_SRC 
There are extra bells and whistles. For one thing, =cases= allows you
to choose the names for the arguments to the constructors using a
=with= clause. In the next example, for example, we choose the name
=m= for the argument to =succ=, so that the second case refers to
=succ m=. More importantly, the cases tactic will detect any items in
the local context that depend on the target variable. It reverts these
elements, does the split, and reintroduces them. In the example below,
notice that the hypothesis =h : n ≠ 0= becomes =h : 0 ≠ 0= in the
first branch, and =h : succ m ≠ 0= in the second.
#+BEGIN_SRC lean
example (n : ℕ) (h : n ≠ 0) : succ (pred n) = n :=
begin
  cases n with m,
  -- first goal: h : 0 ≠ 0 ⊢ succ (pred 0) = 0
    { apply (absurd rfl h) }, 
  -- second goal: h : succ m ≠ 0 ⊢ succ (pred (succ a)) = succ a
  reflexivity
end
#+END_SRC

Notice that =cases= can be used to produce data as well as prove
propositions. 
#+BEGIN_SRC lean
def f (n : ℕ) : ℕ :=
begin
  cases n, exact 3, exact 7
end

example : f 0 = 3 := rfl
example : f 5 = 7 := rfl
#+END_SRC
Once again, cases will revert and depedencies in the context, split,
and then reintroduce them.
#+BEGIN_SRC lean
universe variable u

def tuple (α : Type u) (n : ℕ) := { l : list α // list.length l = n }

variables {α : Type u} {n : ℕ}

def f {n : ℕ} (t : tuple α n) : ℕ :=
begin
  cases n, exact 3, exact 7
end

def my_tuple : tuple ℕ 3 :=  ⟨[0, 1, 2], rfl⟩

example : f my_tuple = 7 := rfl
#+END_SRC

If there are multiple constructors with arguments, you can provide
=cases= with a list of all the names, arranged sequentially:
#+BEGIN_SRC lean
inductive foo : Type
| bar1 : ℕ → ℕ → foo
| bar2 : ℕ → ℕ → ℕ → foo

def silly (x : foo) : ℕ :=
begin
  cases x with a b c d e,
  exact b,    -- a, b, c are in the context
  exact e     -- d, e    are in the context
end
#+END_SRC

You can also use =cases= with an arbitrary expression. Assuming that
expression occurs in the goal, the cases tactic will generalize over
the expression, introduce the resulting universally quantified
variable, and case on that.
#+BEGIN_SRC lean
open nat
variable p : ℕ → Prop

example (hz : p 0) (hs : ∀ n, p (succ n)) (m k : ℕ) : p (m + 3 * k) :=
begin
  cases (m + 3 * k),
  { exact hz },  -- goal is p 0
  apply hs       -- goal is a : ℕ ⊢ p (succ a)
end
#+END_SRC
Think of this as saying "split on cases as to whether =m + 3 * k= is
zero or the successor of some number." The result is functionally
equivalent to the following:
#+BEGIN_SRC lean
open nat
variable p : ℕ → Prop

-- BEGIN
example (hz : p 0) (hs : ∀ n, p (succ n)) (m k : ℕ) : p (m + 3 * k) :=
begin
  generalize (m + 3 * k) n,
  intro n,
  cases n,
  { exact hz },  -- goal is p 0
  apply hs       -- goal is a : ℕ ⊢ p (succ a)
end
-- END
#+END_SRC
Notice that the expression =m + 3 * k= is erased by generalize; all
that matters is whether it is of the form =0= or =succ a=. This form
of =cases= will /not/ revert any hypotheses that also mention the
expression in equation (in this case, =m + 3 * k=). If such a term
appears in a hypothesis and you want to generalize over that as well,
you need to =revert= it explicitly.

If the expression you case on does not appear in the goal, the
=cases= tactic uses =assert= to put the type of the expression into
the context. Here is an example:
#+BEGIN_SRC lean
example (p : Prop) (m n : ℕ) (h₁ : m < n → p) (h₂ : m ≥ n → p) : p :=
begin
  cases lt_or_ge m n with hlt hge,
  { exact h₁ hlt },
  exact h₂ hge
end
#+END_SRC
The theorem =lt_or_ge m n= says =m < n ∨ m ≥ n=, and it is natural to
think of the proof above as splitting on these two cases. In the first
branch, we have the hypothesis =h₁ : m < n=, and in the second we have
the hypothesis =h₂ : m ≥ n=. The proof above is functionally
equivalent to the following:
#+BEGIN_SRC lean
example (p : Prop) (m n : ℕ) (h₁ : m < n → p) (h₂ : m ≥ n → p) : p :=
begin
  assert h : m < n ∨ m ≥ n,
  { exact lt_or_ge m n },
  cases h with hlt hge,
  { exact h₁ hlt },
  exact h₂ hge
end
#+END_SRC
After the first two lines, we have =h : m < n ∨ m ≥ n= as a
hypothesis, and we simply do cases on that.

Here is another example, where we use the decidability of equality on
the natural numbers to split on the cases =m = n= and =m ≠ n=.
#+BEGIN_SRC lean
check nat.sub_self

example (m n : ℕ) : m - n = 0 ∨ m ≠ n :=
begin
  cases decidable.em (m = n) with heq hne,
  { rw heq,
    left, exact nat.sub_self n },
  right, exact hne
end
#+END_SRC
Remember that if you =open classical=, you can use the law of the
excluded middle for any proposition at all. But using type class
inference (see [[file:10_Type_Classes.org::#Type_Classes][Chapter 10]]), Lean can actually find the relevant
decision procedure, which means that you can use the case split in a
computable function.
#+BEGIN_SRC lean
def f (m k : ℕ) : ℕ :=
begin
  cases m - k, exact 3, exact 7
end

example : f 5 7 = 3 := rfl
example : f 10 2 = 7 := rfl
#+END_SRC
Aspects of computability will be discussed in a later chapter.

# TODO(Jeremy): add a reference to this.

# TODO(Jeremy): discuss the induction tactic, and other tactics:
# constructor, contradiction (uses no confusion), etc.


# TODO(Jeremy): this is from Lean 2. Delete?

# You can also use pattern matching in a tactic block. With
# #+BEGIN_SRC lean
# example (p q r : Prop) : p ∧ q ↔ q ∧ p :=
# begin
#   apply iff.intro,
#   { intro h,
#     match h with
#     |  and.intro h₁ h₂ := by apply and.intro; repeat assumption
#     end },
#   { intro h,
#     match h with
#     | and.intro h₁ h₂ := by apply and.intro; repeat assumption
#     end },
# end
# #+END_SRC
# With pattern matching, the first and third examples in this section
# could be written as follows:
# #+BEGIN_SRC lean
# import data.nat
# open nat

# inductive foo : Type :=
# | bar1 : ℕ → ℕ → foo
# | bar2 : ℕ → ℕ → ℕ → foo

# -- BEGIN
# example (x : ℕ) (h : x ≠ 0) : succ (pred x) = x :=
# begin
#   revert h,
#   match x with
#   | 0      := by intro h₁; exact (absurd rfl h₁)
#   | succ y := by intro h₁; apply rfl
#   end
# end

# definition silly (x : foo) : ℕ :=
# begin
#   match x with
#   | foo.bar1 a b   := b
#   | foo.bar2 c d e := e
#   end
# end
# -- END
# #+END_SRC


** Other Inductive Types

Let us consider some more examples of inductively defined types. For
any type, =α=, the type =list α= of lists of elements of =α= is
defined in the library.
#+BEGIN_SRC lean
universe variable u

namespace hide
-- BEGIN
inductive list (α : Type u)
| nil {} : list
| cons : α → list → list

namespace list

variable {α : Type}

notation h :: t  := cons h t

def append (s t : list α) : list α :=
list.rec t (λ x l u, x::u) s

notation s ++ t := append s t

theorem nil_append (t : list α) : nil ++ t = t := rfl

theorem cons_append (x : α) (s t : list α) : x::s ++ t = x::(s ++ t) := rfl

end list
-- END
end hide
#+END_SRC
A list of elements of type =α= is either the empty list, =nil=, or an
element =h : α= followed by a list =t : list α=. We define the
notation =h :: t= to represent the latter. The first element, =h=, is
commonly known as the "head" of the list, and the remainder, =t=, is
known as the "tail." Recall that the notation ={}= in the definition of
the inductive type ensures that the argument to =nil= is implicit. In
most cases, it can be inferred from context. When it cannot, we have to
write =@nil α= to specify the type =α=.

Lean allows us to define iterative notation for lists:
#+BEGIN_SRC lean
universe variable u

namespace hide

-- BEGIN
inductive list (α : Type u)
| nil {} : list
| cons : α → list → list

namespace list

notation `[` l:(foldr `,` (h t, cons h t) nil) `]` := l

section
  open nat
  check [1, 2, 3, 4, 5]
  check ([1, 2, 3, 4, 5] : list num)
end

end list
-- END

end hide
#+END_SRC
In the first =check=, Lean assumes that =[1, 2, 3, 4, 5]= is a list of
natural numbers. The =(t : list num)= expression forces Lean to interpret =t= as
a list of numerals.

As an exercise, prove the following:
#+BEGIN_SRC lean
universe variable u
namespace hide

inductive list (α : Type u)
| nil {} : list
| cons : α → list → list

namespace list

notation `[` l:(foldr `,` (h t, cons h t) nil) `]` := l

variable {α : Type}

notation h :: t  := cons h t

def append (s t : list α) : list α :=
list.rec_on s t (λ x l u, x::u)

notation s ++ t := append s t

theorem nil_append (t : list α) : nil ++ t = t := rfl

theorem cons_append (x : α) (s t : list α) : x::s ++ t = x::(s ++ t) := rfl

-- BEGIN
theorem append_nil (t : list α) : t ++ nil = t := sorry

theorem append_assoc (r s t : list α) : r ++ s ++ t = r ++ (s ++ t) := sorry
-- END

end list

end hide
#+END_SRC
Try also defining the function =length : Π α : Type, list α → nat=
that returns the length of a list, and prove that it behaves as
expected (for example, =length (s ++ t) = length s + length t=).

For another example, we can define the type of binary trees:
#+BEGIN_SRC lean
inductive binary_tree
| leaf : binary_tree
| node : binary_tree → binary_tree → binary_tree
#+END_SRC
In fact, we can even define the type of countably branching trees:
#+BEGIN_SRC lean
inductive cbtree
| leaf : cbtree
| sup : (ℕ → cbtree) → cbtree

namespace cbtree

def succ (t : cbtree) : cbtree :=
sup (λ n, t)

def omega : cbtree :=
sup (λ n, nat.rec_on n leaf (λ n t, succ t))

end cbtree
#+END_SRC

# TODO (JDA): I got tired here, but more can be ported from the parts
# that are commented out below.

# ** Generalizations

# We now consider two generalizations of inductive types that
# are sometimes useful. First, Lean supports /mutually defined inductive
# types/. The idea is that we can define two (or more) inductive types
# at the same time, where each one refers to the other.

# #+BEGIN_SRC lean
# inductive tree (α : Type) : Type :=
# | node : α → forest α → tree α
# with forest : Type :=
# | nil  : forest α
# | cons : tree α → forest α → forest α
# #+END_SRC
# In this example, a =tree= with elements labeled from =α= is of the
# form =node a f=, where =a= is an element of =α= (the label), and =f= a
# forest. At the same time, a =forest= of trees with elements labeled
# from =α= is essentially defined to be a list of trees.

# A more powerful generalization is given by the possibility of defining
# inductive type =families=. There are indexed families of types defined
# by a simultaneous induction of the following form:
# #+BEGIN_SRC text
# inductive foo : ... → Type :=
# | constructor₁ : ... → foo ...
# | constructor₂ : ... → foo ...
# ...
# | constructorₙ : ... → foo ...
# #+END_SRC
# In contrast to ordinary inductive definition, which construct an
# element of =Type=, the more general version constructs a function
# =... → Type=, where "=...=" denotes a sequence of argument types, also
# known as /indices/. Each constructor then constructs an element of some
# type in the family. One example is the definition of =vector α n=, the
# type of vectors of elements of =α= of length =n=:
# #+BEGIN_SRC lean
# open nat
# namespace hide

# -- BEGIN
# inductive vector (α : Type) : nat → Type :=
# | nil {} : vector α zero
# | cons   : Π {n}, α → vector α n → vector α (succ n)
# -- END

# end hide
# #+END_SRC
# Notice that the =cons= constructor takes an element of =vector α n=,
# and returns an element of =vector α (succ n)=, thereby using an
# element of one member of the family to build an element of another.

# Another example is given by the family of types =fin n=. For each =n=,
# =fin n= is supposed to denote a generic type of =n= elements:
# #+BEGIN_SRC lean
# namespace hide

# -- BEGIN
# inductive fin : nat → Type :=
# | fz : Π n, fin (nat.succ n)
# | fs : Π {n}, fin n → fin (nat.succ n)
# -- END

# end hide
# #+END_SRC
# This example may be hard to understand, so you should take the time to
# think about how it works.

# Yet another example is given by the definition of the equality type in
# the library:
# #+BEGIN_SRC lean
# namespace hide

# -- BEGIN
# inductive eq {α : Type} (a : α) : α → Prop :=
# refl : eq a a
# -- END

# end hide
# #+END_SRC
# For each fixed =α : Type= and =a : α=, this definition constructs a
# family of types =eq a x=, indexed by =x : α=. Notably, however, there
# is only one constructor, =refl=, which is an element of =eq a
# a=. Intuitively, the only way to construct a proof of =eq a x= is to
# use reflexivity, in the case where =x= is =a=.  Note that =eq a a= is
# the only inhabited type in the family of types =eq a x=.  The
# elimination principle generated by Lean says that =eq= is the /least/
# reflexive relation on =α=. The eliminator/recursor for =eq= is of the
# following form:
# #+BEGIN_SRC text
# eq.rec_on : Π {α : Type} {a : α} {C : α → Type} {b : α}, a = b → C a → C b
# #+END_SRC
# It is a remarkable fact that all the basic axioms for equality follow
# from the constructor, =refl=, and the eliminator, =eq.rec_on=.

# This eliminator illustrates the exception to the fact
# that inductive definitions living in =Prop= can only eliminate to
# =Prop=. Because there is only one constructor to =eq=, it carries no
# information, other than the type is inhabited, and Lean's internal
# logic allows us to eliminate to an arbitrary =Type=. This is how we
# define a /cast/ operation that casts an element from type =α= into =β=
# when a proof =p : eq α β= is provided:
# #+BEGIN_SRC lean
# namespace hide

# inductive eq {α : Type} (a : α) : α → Prop :=
# refl : eq a a

# -- BEGIN
# theorem cast {α β : Type} (p : eq α β) (a : α) : β :=
# eq.rec_on p a
# -- END

# end hide
# #+END_SRC

# The recursor =eq.rec_on= is also used to define substitution:
# #+BEGIN_SRC lean
# namespace hide

# inductive eq {α : Type} (a : α) : α → Prop :=
# refl : eq a a

# -- BEGIN
# theorem subst {α : Type} {a b : α} {P : α → Prop}
#   (H₁ : eq a b) (H₂ : P a) : P b :=
# eq.rec H₂ H₁
# -- END

# end hide
# #+END_SRC
# Using the recursor with =H₁ : a = b=, we may assume =a= and =b= are
# the same, in which case, =P b= and =P a= are the same.

# It is not hard to prove that =eq= is symmetric and transitive.
# In the following example, we prove =symm= and leave as exercise
# the theorems =trans= and =congr= (congruence).

# #+BEGIN_SRC lean
# namespace hide

# inductive eq {α : Type} (a : α) : α → Prop :=
# refl : eq a a

# theorem subst {α : Type} {a b : α} {P : α → Prop}
#   (H₁ : eq a b) (H₂ : P a) : P b :=
# eq.rec H₂ H₁

# -- BEGIN
# theorem symm {α : Type} {a b : α} (H : eq a b) : eq b a :=
# subst H (eq.refl a)

# theorem trans {α : Type} {a b c : α} (H₁ : eq a b) (H₂ : eq b c) : eq a c :=
# sorry

# theorem congr {α β : Type} {a b : α} (f : α → β) (H : eq a b) : eq (f a) (f b) :=
# sorry
# -- END

# end hide
# #+END_SRC

# In the type theory literature, there are further generalizations of
# inductive definitions, for example, the principles of
# /induction-recursion/ and /induction-induction/.  These are not
# supported by Lean.

# TODO(Jeremy): Move this to a later chapter on fine points of
# dependent type theory.

# ** Heterogeneous Equality

# Given =α : Type= and =β : α → Type=, suppose we want to generalize the
# congruence theorem =congr= in the previous example to dependent
# functions =f : Π x : α, β x=. Roughly speaking, we would like to have
# a theorem that, says that if =a = b=, then =f a = f b=. The first
# obstacle is stating the theorem: the term =eq (f a) (f b)= is not type
# correct since =f a= has type =β a=, =f b= has type =β b=, and the
# equality predicate =eq= expects both arguments to have the same
# type. Notice that =f a= has type =β a=, so the term =eq.rec_on H (f
# a)= has type =β b=. You should think of =eq.rec_on H (f a)= as "=f a=,
# viewed as an element of =β b=." We can then write =eq (eq.rec_on H (f a))
# (f b)= to express that =f a= and =f b= are equal, modulo the
# difference between their types. Here is a proof of the generalized
# congruence theorem, with this approach:
# #+BEGIN_SRC lean
# namespace hide

# inductive eq {α : Type} (a : α) : α → Prop :=
# refl : eq a a

# -- BEGIN
# theorem hcongr {α : Type} {β : α → Type} {a b : α} (f : Π x : α, β x)
#                (H : eq a b) : eq (eq.rec_on H (f a)) (f b) :=
# have h₁ : ∀ h : eq a a, eq (eq.rec_on h (f a)) (f a), from
#   assume h : eq a a, eq.refl (eq.rec_on h (f a)),
# have h₂ : ∀ h : eq a b, eq (eq.rec_on h (f a)) (f b), from
#   eq.rec_on H h₁,
# show eq (eq.rec_on H (f a)) (f b), from
#   h₂ H
# -- END

# end hide
# #+END_SRC

# Another option is to define a /heterogeneous equality/ =heq= that can
# equate terms of different types, so that we can write =heq (f a) (f
# b)= instead of =eq (eq.rec_on H (f a)) (f b)=. It is straightforward
# to define such an equality in Lean:
# #+BEGIN_SRC lean
# namespace hide

# -- BEGIN
# inductive heq {α : Type} (a : α) : Π {β : Type}, β → Prop :=
# refl : heq a a
# -- END

# end hide
# #+END_SRC
# Moreover, given =a b : α=, we can prove =heq a b → eq a b= using proof
# irrelevance.  This theorem is called =heq.to_eq= in the Lean standard
# library. We can now state and prove =hcongr= using heterogeneous
# equality. Note the proof is also more compact and easier to
# understand.
# #+BEGIN_SRC lean
# namespace hide

# inductive eq {α : Type} (a : α) : α → Prop :=
# refl : eq a a

# inductive heq {α : Type} (a : α) : Π {β : Type}, β → Prop :=
# refl : heq a a

# -- BEGIN
# theorem hcongr {α : Type} {β : α → Type} {a b : α} (f : Π x : α, β x)
#                (H : eq a b) : heq (f a) (f b) :=
# eq.rec_on H (heq.refl (f a))
# -- END

# end hide
# #+END_SRC
# Heterogeneous equality, which gives elements of different types the
# illusion that they can be considered equal, is sometimes called /John
# Major equality/. (The name is a bit of political humor, due to Conor
# McBride.)

# ** Automatically Generated Constructions
# :PROPERTIES:
#   :CUSTOM_ID: Automatically_Generated_Constructions
# :END:

# In the previous sections, we have seen that whenever we declare an
# inductive data type =I=, the Lean kernel automatically declares its
# constructors (aka introduction rules), and generates and declares the
# eliminator/recursor =I.rec=. The eliminator expresses a principle of
# definition by recursion, as well as the principle of proof by
# induction. The kernel also associates a /computational rule/ which
# determines how these definitions are eliminated when terms and proofs
# are normalized.

# Consider, for example, the natural numbers. Given the motive =C : nat
# → Type=, and minor premises =fz : C zero= and =fs : Π (n : nat), C n →
# C (succ n)=, we have the following two computational rules: =nat.rec
# fz fs zero= reduces to =fz=, and =nat.rec fz fs (succ a)= reduces to
# =fs a (nat.rec fz fs a)=.
# #+BEGIN_SRC lean
# open nat

# variable C  : nat → Type
# variable fz : C zero
# variable fs : Π (n : nat), C n → C (succ n)

# eval nat.rec fz fs zero
# -- nat.rec_on is defined from nat.rec
# eval nat.rec_on zero fz fs

# example : nat.rec fz fs zero = fz :=
# rfl

# variable a : nat

# eval nat.rec fz fs (succ a)
# eval nat.rec_on (succ a) fz fs

# example (a : nat) : nat.rec fz fs (succ a) = fs a (nat.rec fz fs a) :=
# rfl
# #+END_SRC
# The source code that validates an inductive declaration and generates
# the eliminator/recursor and computational rules is part of the Lean
# kernel. The kernel is also known as the /trusted code base/, because a
# bug in the kernel may compromise the soundness of the whole system.

# When you define an inductive data type, Lean automatically generates a
# number of useful definitions. We have already seen some of them:
# =rec_on=, =induction_on=, and =cases_on=. The module =M= that
# generates these definitions is /not/ part of the trusted code base. A
# bug in =M= does not compromise the soundness of the whole system,
# since the kernel will catch such errors when type checking any
# incorrectly generated definition produced by =M=.

# As described before, =rec_on= just uses its arguments in a more
# convenient order than =rec=. In =rec_on=, the major premise is
# provided before the minor premises. Constructions using =rec_on= are
# often easier to read and understand than the equivalent ones using
# =rec=.
# #+BEGIN_SRC lean
# open nat

# print definition nat.rec_on

# definition rec_on {C : nat → Type} (n : nat)
#                   (fz : C zero) (fs : Π a, C a → C (succ a)) : C n :=
# nat.rec fz fs n
# #+END_SRC
# Moreover, =induction_on= is just a special case of =rec_on= where the
# motive =C= is a proposition. Finally, =cases_on= is a special case of
# =rec_on= where the inductive/recursive hypotheses are omitted in the
# minor premises. For example, in =nat.cases_on= the minor premise =fs=
# has type =Π (n : nat), C (succ n)= instead of =Π (n : nat), C n → C
# (succ n)=. Note that the inductive/recursive hypothesis =C n= has
# been omitted.

# #+BEGIN_SRC lean
# namespace hide
# -- BEGIN
# open nat

# print definition nat.induction_on
# print definition nat.cases_on

# definition induction_on {C : nat → Prop} (n : nat)
#                         (fz : C zero) (fs : Π a, C a → C (succ a)) : C n :=
# nat.rec_on n fz fs

# definition cases_on {C : nat → Prop} (n : nat)
#                     (fz : C zero) (fs : Π a, C (succ a)) : C n :=
# nat.rec_on n fz (fun (a : nat) (r : C a), fs a)
# -- END
# end hide
# #+END_SRC

# For any inductive data type that is not a proposition, we can show that
# its constructors are injective and disjoint. For example, on =nat=, we
# can show that =succ a = succ b → a = b= (injectivity), and =succ a ≠
# zero= (disjointness). Both proofs can be performed using the
# automatically generated definition =nat.no_confusion=. More generally,
# for any inductive data type =I= that is not a proposition, Lean
# automatically generates a definition of =I.no_confusion=. Given a
# motive =C= and an equality =h : c₁ t = c₂ s=, where =c₁= and =c₂= are
# two distinct =I= constructors, =I.no_confusion= constructs an
# inhabitant of =C=.  This is essentially the /principle of explosion/,
# that is, the fact that anything follows from a contradiction. On the
# other hand, given a proof of =c t = c s= with the same constructor on
# both sides and a proof of =t = s → C=, =I.no_confusion= returns an
# inhabitant of =C=.

# Let us illustrate by considering the constructions for the type =nat=.
# The type of =no_confusion= is based on the auxiliary definition
# =no_confusion_type=:
# #+BEGIN_SRC lean
# open nat

# check @nat.no_confusion
# -- Π {P : Type} {v1 v2 : ℕ}, v1 = v2 → nat.no_confusion_type P v1 v2

# check nat.no_confusion_type
# -- Type → ℕ → ℕ → Type
# #+END_SRC
# Note that the motive is an implicit argument in =no_confusion=. The
# constructions work as follows:
# #+BEGIN_SRC lean
# open nat
# -- BEGIN
# variable C : Type
# variables a b : nat

# eval nat.no_confusion_type C zero     (succ a)
# -- C
# eval nat.no_confusion_type C (succ a) zero
# -- C
# eval nat.no_confusion_type C zero     zero
# -- C → C
# eval nat.no_confusion_type C (succ a) (succ b)
# -- (a = b → C) → C
# -- END
# #+END_SRC
# In other words, from a proof of =zero = succ a= or =succ a = 0=, we
# obtain an element of any type =C= at will. On the other hand, a proof
# of =zero = zero= provides no help in constructing an element of type
# =C=, whereas a proof of =succ a = succ b= reduces the task of
# constructing an element of type =C= to the task of constructing such
# an element under the additional hypothesis =a = b=.

# It is not hard to prove that constructors are injective and disjoint
# using =no_confusion=.  In the following example, we prove these two
# properties for =nat= and leave as exercise the equivalent proofs for
# trees.
# #+BEGIN_SRC lean
# open nat

# theorem succ_ne_zero (a : nat) (h : succ a = zero) : false :=
# nat.no_confusion h

# theorem succ.inj (a b : nat) (h : succ a = succ b) : a = b :=
# nat.no_confusion h (fun e : a = b, e)

# inductive tree (α : Type) : Type :=
# | leaf : α → tree α
# | node : tree α → tree α → tree α

# open tree

# variable {α : Type}

# theorem leaf_ne_node {a : α} {l r : tree α}
#                      (h : leaf a = node l r) : false :=
# sorry

# theorem leaf_inj {a b : α} (h : leaf a = leaf b) : a = b :=
# sorry

# theorem node_inj_left {l1 r1 l2 r2 : tree α}
#                       (h : node l1 r1 = node l2 r2) : l1 = l2 :=
# sorry

# theorem node_inj_right {l1 r1 l2 r2 : tree α}
#                        (h : node l1 r1 = node l2 r2) : r1 = r2 :=
# sorry
# #+END_SRC

# If a constructor contains dependent arguments (such as =sigma.mk=),
# the generated =no_confusion= uses heterogeneous equality to equate
# arguments of different types:
# #+BEGIN_SRC lean
# variables (α : Type) (β : α → Type)
# variables (a1 a2 : α) (b1 : β a1) (b2 : β a2)
# variable  (C : Type)

# -- Remark: b1 and b2 have different types

# eval sigma.no_confusion_type C (sigma.mk a1 b1) (sigma.mk a2 b2)
# -- (a1 = a2 → b1 == b2 → C) → C
# #+END_SRC

# Lean also generates the predicate transformer =below= and the recursor
# =brec_on=. It is unlikely that you will ever need to use these
# constructions directly; they are auxiliary definitions used by the
# recursive equation compiler we will describe in the next chapter, and
# we will not discuss them further here.

# ** Universe Levels

# Since an inductive type lives in =Type.{i}= for some =i=, it is
# reasonable to ask /which/ universe levels =i= can be instantiated
# to. The goal of this section is to explain the relevant constraints.

# In the standard library, there are two cases, depending on whether the
# inductive type is specified to land in =Prop=. Let us first consider
# the case where the inductive type is not specified to land in =Prop=,
# which is the only case that arises in the homotopy type theory
# instantiation of the kernel. Recall that each constructor =c= in the
# definition of a family =C= of inductive types is of the form
# #+BEGIN_SRC text
# c : Π (a : α) (b : β[a]), C a p[a,b]
# #+END_SRC
# where =a= is a sequence of data type parameters, =b= is the sequence of
# arguments to the constructors, and =p[a, b]= are the indices, which
# determine which element of the inductive family the construction
# inhabits. Then the universe level =i= of =C= is constrained to satisfy
# the following:
# #+BEGIN_QUOTE
# For each constructor =c= as above, and each =βk[a]= in the sequence
# =β[a]=, if =βk[a] : Type.{j}=, we have =i= ≥ =j=.
# #+END_QUOTE
# In other words, the universe level =i= is required to be at least as
# large as the universe level of each type that represents an argument
# to a constructor.

# When the inductive type =C= is specified to land in =Prop=, there are
# no constraints on the universe levels of the constructor
# arguments. But these universe levels do have a bearing on the
# elimination rule. Generally speaking, for an inductive type in =Prop=,
# the motive of the elimination rule is required to be in =Prop=. The
# exception we alluded to in the discussion of equality above is this:
# we are allowed to eliminate to an arbitrary =Type= when there is only
# one constructor, and each constructor argument is either in =Prop= or
# an index. This exception, which makes it possible to treat ordinary
# equality and heterogeneous equality as inductive types, can be
# justified by the fact that the elimination rule cannot take advantage
# of any "hidden" information.

# Because inductive types can be polymorphic over universe levels,
# whether an inductive definition lands in =Prop= could, in principle,
# depend on how the universe levels are instantiated. To simplify the
# generation of the recursors, Lean adopts a convention that rules out
# this ambiguity: if you do not specify that the inductive type is an
# element of =Prop=, Lean requires the universe level to be at least
# one. Hence, a type specified by single inductive definition is either
# always in =Prop= or never in =Prop=. For example, if =α= and =β= are
# elements of =Prop=, =α × β= is assumed to have universe level at least
# one, representing a data type rather than a proposition. The analogous
# definition of =α × β=, where =α= and =β= are restricted to =Prop= and
# the resulting type is declared to be an element of =Prop= instead of
# =Type=, is exactly the definition of =α ∧ β=.
